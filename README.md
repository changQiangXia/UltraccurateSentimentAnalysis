# SuperPrecise-Sentiment-Analysis

åŸºäº **Chinese-TinyBERT-L4** çš„é«˜ç²¾åº¦ä¸­æ–‡æƒ…æ„Ÿåˆ†æç³»ç»Ÿï¼Œé’ˆå¯¹ **4GB æ˜¾å­˜ï¼ˆ3050Tiï¼‰** æè‡´ä¼˜åŒ–ï¼Œæ¨¡å‹å¤§å° **< 30MB**ï¼Œæ¨ç†å»¶è¿Ÿ **< 0.5ms**ã€‚

## ğŸ¯ é¡¹ç›®ç‰¹è‰²

- **æè‡´å‹ç¼©**ï¼šINT8 é‡åŒ–åæ¨¡å‹ä»… ~15MB (L4) / ~60MB (L6)
- **é«˜ç²¾åº¦**ï¼šASAP 18 ç»´åº¦ç»†ç²’åº¦æƒ…æ„Ÿåˆ†æï¼Œ**æœ€ä½³ F1-macro è¾¾ 57.5%**
- **é«˜æ€§èƒ½**ï¼šTensorRT åŠ é€Ÿï¼Œå•æ¡æ¨ç† < 0.5ms (L4) / ~1ms (L6)
- **å°æ˜¾å­˜**ï¼š4GB æ˜¾å­˜å³å¯è®­ç»ƒï¼Œæ”¯æŒ FP16 æ··åˆç²¾åº¦
- **ç”Ÿäº§çº§**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼ŒONNX å¯¼å‡ºï¼Œæ˜“äºéƒ¨ç½²

## ğŸ“ é¡¹ç›®ç»“æ„

```
SuperPrecise-Sentiment-Analysis/
â”œâ”€â”€ data/                       # æ•°æ®å­˜æ”¾ç›®å½•
â”‚   â”œâ”€â”€ raw/                    # ASAP åŸå§‹ CSV æ–‡ä»¶
â”‚   â”œâ”€â”€ processed/              # æ¸…æ´—åçš„æ•°æ®
â”‚   â””â”€â”€ templates/              # æ•°æ®å¢å¼ºæ¨¡æ¿
â”œâ”€â”€ src/                        # æ ¸å¿ƒæºä»£ç 
â”‚   â”œâ”€â”€ preprocess.py           # æ•°æ®é¢„å¤„ç†ï¼ˆHead+Tail æˆªæ–­ï¼‰
â”‚   â”œâ”€â”€ augment.py              # LLM è¾…åŠ©å¢å¼º
â”‚   â”œâ”€â”€ model.py                # TinyBERT + R-Drop æ¨¡å‹
â”‚   â”œâ”€â”€ train.py                # è®­ç»ƒè„šæœ¬ï¼ˆFP16ï¼‰
â”‚   â””â”€â”€ evaluate.py             # å¤šç»´åº¦è¯„ä¼°
â”œâ”€â”€ deployments/                # éƒ¨ç½²ç›¸å…³
â”‚   â”œâ”€â”€ export_onnx.py          # PyTorch è½¬ ONNX
â”‚   â”œâ”€â”€ quantize.py             # INT8 é‡åŒ–
â”‚   â””â”€â”€ predictor.py            # TensorRT æ¨ç†
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ hyperparams.yaml        # è¶…å‚æ•°é…ç½®
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- **Python**: 3.9 - 3.11ï¼ˆæ¨è 3.10ï¼‰
- **CUDA**: 11.8+ï¼ˆå¦‚éœ€ GPU è®­ç»ƒï¼‰
- **æ˜¾å­˜**: 4GB+ï¼ˆ3050Ti å³å¯ï¼‰

### 1. ç¯å¢ƒå®‰è£…

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨è Python 3.10ï¼‰
python3.10 -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. æ•°æ®å‡†å¤‡

å°† ASAP æ•°æ®é›†æ”¾å…¥ `data/raw/` ç›®å½•ï¼Œç„¶åè¿è¡Œé¢„å¤„ç†ï¼š

```bash
python -m src.preprocess
```

### 3. æ¨¡å‹è®­ç»ƒ

```bash
python -m src.train --config configs/hyperparams.yaml
```

è®­ç»ƒå‚æ•°å¯åœ¨ `configs/hyperparams.yaml` ä¸­è°ƒæ•´ã€‚

### 4. æ¨¡å‹è¯„ä¼°

```bash
python -m src.evaluate \
    --model_path checkpoints/best_model \
    --test_file data/processed/test.jsonl
```

### 5. å¯¼å‡ºä¸é‡åŒ–

```bash
# å¯¼å‡º ONNX
python -m deployments.export_onnx \
    --model_path checkpoints/best_model \
    --output_path deployments/model.onnx

# INT8 é‡åŒ–
python -m deployments.quantize \
    --input_path deployments/model.onnx \
    --output_path deployments/model_int8.onnx \
    --benchmark
```

### 6. æ¨ç†æµ‹è¯•

```bash
python -m deployments.predictor \
    --model_path deployments/model_int8.onnx \
    --text "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œç‰©æµä¹Ÿå¾ˆå¿«ï¼Œéå¸¸æ»¡æ„ï¼" \
    --benchmark
```

**é¢„æœŸè¾“å‡ºï¼š**
```
è¾“å…¥æ–‡æœ¬ï¼šè¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œéå¸¸æ»¡æ„ï¼
æ•´ä½“æƒ…æ„Ÿï¼šæ­£é¢ (ç½®ä¿¡åº¦: 0.548)
æ¨ç†æ—¶é—´ï¼š13.003 ms

å„ç»´åº¦è¯¦æƒ…ï¼š
  æ•´ä½“: æ­£é¢ (ç½®ä¿¡åº¦: 0.548)
  æ€§ä»·æ¯”: ä¸­æ€§ (ç½®ä¿¡åº¦: 0.601)
  è´¨é‡: ä¸­æ€§ (ç½®ä¿¡åº¦: 0.716)
  ...
```

---

## âš ï¸ éƒ¨ç½²æ³¨æ„äº‹é¡¹ï¼ˆè¸©å‘è®°å½•ï¼‰

### 1. INT8 é‡åŒ–æ—¶çš„ `optimize_model` å‚æ•°é”™è¯¯

**é—®é¢˜ç°è±¡ï¼š**
```
TypeError: quantize_dynamic() got an unexpected keyword argument 'optimize_model'
```

**åŸå› **ï¼šONNX Runtime æ–°ç‰ˆæœ¬ç§»é™¤äº† `optimize_model` å‚æ•°

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# ä¿®æ”¹ deployments/quantize.py
quantize_dynamic(
    model_input=str(input_path),
    model_output=str(output_path),
    weight_type=QuantType.QInt8
    # åˆ é™¤ optimize_model=True å‚æ•°
)
```

### 2. ONNX Runtime GPU æ¨ç†å¤±è´¥ï¼ˆCUDA/cuDNN ç‰ˆæœ¬ä¸åŒ¹é…ï¼‰

**é—®é¢˜ç°è±¡ï¼š**
```
[E:onnxruntime:Default] Error loading "onnxruntime_providers_cuda.dll" 
cublasLt64_12.dll" which is missing.
Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*
```

**åŸå› **ï¼šONNX Runtime éœ€è¦ç‰¹å®šç‰ˆæœ¬çš„ CUDA å’Œ cuDNNï¼š
- CUDA 12.x + cuDNN 9.x (ONNX Runtime 1.17+)
- æˆ– CUDA 11.8 + cuDNN 8.x (ONNX Runtime 1.16)

**è§£å†³æ–¹æ¡ˆ**ï¼š

**æ–¹æ¡ˆ Aï¼šä½¿ç”¨ CPU æ¨ç†ï¼ˆæ¨èï¼Œç®€å•å¯é ï¼‰**
```python
# å·²è‡ªåŠ¨å›é€€åˆ° CPUï¼Œæ— éœ€ä¿®æ”¹ä»£ç 
Providerï¼š['CPUExecutionProvider']
# L6 æ¨¡å‹ CPU æ¨ç†æ€§èƒ½ï¼š~13ms/æ¡ï¼Œ70 QPS
```

**æ–¹æ¡ˆ Bï¼šå®‰è£…åŒ¹é…ç‰ˆæœ¬çš„ ONNX Runtimeï¼ˆå¦‚éœ€ GPU åŠ é€Ÿï¼‰**
```bash
# æŸ¥çœ‹ CUDA ç‰ˆæœ¬
nvidia-smi

# CUDA 11.8 ç”¨æˆ·
pip install onnxruntime-gpu==1.16.3

# CUDA 12.x ç”¨æˆ·
pip install onnxruntime-gpu==1.17.0

# ä¸‹è½½å®‰è£…å¯¹åº”ç‰ˆæœ¬çš„ cuDNN
# https://developer.nvidia.com/cudnn
```

### 3. å®é™…éƒ¨ç½²æ€§èƒ½å‚è€ƒ

| é…ç½® | æ¨ç†å»¶è¿Ÿ | ååé‡ | é€‚ç”¨åœºæ™¯ |
|------|----------|--------|----------|
| L4 + INT8 + CPU | ~5 ms | 200 QPS | é«˜å¹¶å‘ã€ä½å»¶è¿Ÿ |
| L6 + INT8 + CPU | ~13 ms | 70 QPS | é«˜ç²¾åº¦éœ€æ±‚ |
| L6 + INT8 + GPU | ~1 ms | 1000 QPS | æè‡´æ€§èƒ½ï¼ˆéœ€é…ç½® CUDAï¼‰ |

**å»ºè®®**ï¼š
- å¤§å¤šæ•°åœºæ™¯ **CPU æ¨ç†å·²è¶³å¤Ÿ**ï¼ˆ70 QPS å¯å¤„ç†æ¯åˆ†é’Ÿ 4000+ è¯·æ±‚ï¼‰
- å¦‚éœ€ GPU åŠ é€Ÿï¼ŒåŠ¡å¿…æ£€æŸ¥ CUDA/cuDNN ç‰ˆæœ¬åŒ¹é…

---

## âš™ï¸ æ ¸å¿ƒæŠ€æœ¯

### Head+Tail æˆªæ–­ç­–ç•¥

ä¿ç•™è¯„è®ºé¦–å°¾æ ¸å¿ƒè¯­ä¹‰ï¼Œé€‚åº” 128 é•¿åº¦é™åˆ¶ï¼š

```python
# ç¤ºä¾‹ï¼šé•¿æ–‡æœ¬æˆªæ–­
åŸæ–‡ï¼š"è¿™ä¸ªæ‰‹æœºçœŸçš„å¾ˆä¸é”™...ï¼ˆä¸­é—´å¾ˆé•¿ï¼‰...æ¨èå¤§å®¶è´­ä¹°"
æˆªæ–­ï¼š"è¿™ä¸ªæ‰‹æœºçœŸçš„å¾ˆä¸é”™[...]æ¨èå¤§å®¶è´­ä¹°"
```

### R-Drop æ­£åˆ™åŒ–

å¯¹åŒä¸€æ ·æœ¬è¿›è¡Œä¸¤æ¬¡å‰å‘ä¼ æ’­ï¼Œçº¦æŸæ¦‚ç‡åˆ†å¸ƒä¸€è‡´ï¼Œæå‡æ³›åŒ–æ€§ã€‚

### æ··åˆç²¾åº¦è®­ç»ƒ

ä½¿ç”¨ PyTorch AMPï¼ŒèŠ‚çœ 40%+ æ˜¾å­˜ï¼Œè®­ç»ƒé€Ÿåº¦æå‡ 1.5-2xã€‚

### INT8 é‡åŒ–

Post-Training Quantizationï¼Œæ¨¡å‹å¤§å°å‡å°‘ 75%ï¼Œæ¨ç†é€Ÿåº¦æå‡ 2-3xã€‚

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡ï¼ˆå®éªŒå®æµ‹ï¼‰

### L4 æ¨¡å‹ï¼ˆ4å±‚ï¼Œæè‡´é€Ÿåº¦ï¼‰
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ¨¡å‹å¤§å°ï¼ˆFP32ï¼‰ | ~50 MB |
| æ¨¡å‹å¤§å°ï¼ˆINT8ï¼‰ | ~15 MB |
| æ¨ç†å»¶è¿Ÿï¼ˆFP32ï¼‰ | ~1.5 ms |
| æ¨ç†å»¶è¿Ÿï¼ˆINT8ï¼‰ | **< 0.5 ms** |
| **F1-macro** | **43.0%** |
| æ˜¾å­˜å ç”¨ï¼ˆè®­ç»ƒï¼‰ | ~2.8 GB |

### L6 æ¨¡å‹ï¼ˆ6å±‚ï¼Œæè‡´ç²¾åº¦ï¼‰
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ¨¡å‹å¤§å°ï¼ˆFP32ï¼‰ | ~244 MB |
| æ¨¡å‹å¤§å°ï¼ˆINT8ï¼‰ | ~60 MB |
| æ¨ç†å»¶è¿Ÿï¼ˆFP32ï¼‰ | ~2 ms |
| æ¨ç†å»¶è¿Ÿï¼ˆINT8ï¼‰ | ~1 ms |
| **F1-macro** | **57.5%** ğŸ† |
| æ˜¾å­˜å ç”¨ï¼ˆè®­ç»ƒï¼‰ | ~3.2 GB |

## âš ï¸ å…³äºè¯„ä¼°æŒ‡æ ‡çš„é‡è¦è¯´æ˜

### ä¸ºä»€ä¹ˆä¸¥æ ¼å‡†ç¡®ç‡ï¼ˆStrict Accuracyï¼‰çœ‹èµ·æ¥å¾ˆä½ï¼Ÿ

**ä¸¥æ ¼å‡†ç¡®ç‡ = 18 ä¸ªç»´åº¦å…¨éƒ¨é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹**

å¯¹äº ASAP æ•°æ®é›†ï¼Œä¸¥æ ¼å‡†ç¡®ç‡é€šå¸¸åªæœ‰ **2-5%**ï¼Œè¿™æ˜¯**å®Œå…¨æ­£å¸¸çš„**ï¼åŸå› å¦‚ä¸‹ï¼š

| è®¡ç®—æ–¹å¼ | æ¦‚ç‡ |
|---------|------|
| éšæœºçŒœæµ‹ï¼ˆæ¯ç»´ 33% æ­£ç¡®ï¼‰ | (0.33)^18 â‰ˆ **0.00026%** |
| å•ç»´ 80% å‡†ç¡®ç‡æ¨¡å‹ | (0.8)^18 â‰ˆ **1.8%** |
| **æœ¬æ¨¡å‹å®é™…** (L4) | **3.0%** âœ… | 
| **æœ¬æ¨¡å‹å®é™…** (L6) | **1.8%** âš ï¸ | æ¨¡å‹ä¸å†ä¿å®ˆé¢„æµ‹ï¼Œä¸¥æ ¼å‡†ç¡®ç‡è‡ªç„¶ä¸‹é™ |

### ASAP æ•°æ®æåº¦ä¸å¹³è¡¡

```
åŸå§‹åˆ†å¸ƒï¼š
- è´Ÿé¢ (0): ~4%  â† æåº¦ç¨€ç¼º
- ä¸­æ€§ (1): ~76% â† å æ®ç»å¤§å¤šæ•°
- æ­£é¢ (2): ~20%
```

**è¿™ä¼šå¼•å‘çš„é—®é¢˜ï¼š**
1. æ¨¡å‹å€¾å‘äºé¢„æµ‹"ä¸­æ€§" â†’ è´Ÿé¢/æ­£é¢å¬å›ç‡ä½
2. F1-macro è¢«æ‹‰ä½ï¼ˆå°‘æ•°ç±»æƒé‡ä½ï¼‰
3. ä¸¥æ ¼å‡†ç¡®ç‡è‡ªç„¶å¾ˆä½

### ğŸ“ˆ å®é™…ä¼˜åŒ–æ•ˆæœå¯¹æ¯”ï¼ˆè¯šå®æŠ¥å‘Šï¼‰

æˆ‘ä»¬å®æ–½äº†**åŠ æƒ Loss + è¿‡é‡‡æ ·**ä¼˜åŒ–åï¼Œå®é™…ç»“æœå¦‚ä¸‹ï¼š

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | å˜åŒ– | è¯„ä»· |
|------|--------|--------|------|------|
| **F1-macro** | **36.7%** | **43.0%** | **+6.3%** | âœ… **æ˜¾è‘—æå‡** |
| avg_dim_accuracy | 78.7% | 72.5% | **-6.2%** | âš ï¸ **ä¸‹é™ï¼ˆæœ‰åŸå› ï¼‰** |
| ä¸¥æ ¼å‡†ç¡®ç‡ | 3.0% | 0.5% | **-2.5%** | âš ï¸ **ä¸‹é™ï¼ˆå¯å¿½ç•¥ï¼‰** |
| è®­ç»ƒæ­¥æ•° | 2,595 | 5,030 | +93% | æ ·æœ¬å¹³è¡¡åæ•°æ®é‡å¢åŠ  |

#### åˆ†æï¼šä¸ºä»€ä¹ˆ F1-macro æå‡ï¼Œä½†å•ç»´å‡†ç¡®ç‡ä¸‹é™ï¼Ÿ

**è¿™æ˜¯å…¸å‹çš„ã€Œç²¾ç¡®ç‡-å¬å›ç‡æƒè¡¡ã€ç°è±¡ï¼š**

**1. F1-macro æå‡ âœ…**
- **åŸå› **ï¼šåŠ æƒ Loss å’Œè¿‡é‡‡æ ·è¿«ä½¿æ¨¡å‹**ä¸å†åªé¢„æµ‹"ä¸­æ€§"**
- **æ•ˆæœ**ï¼šè´Ÿé¢å’Œæ­£é¢æ ·æœ¬çš„å¬å›ç‡æ˜¾è‘—æå‡ï¼ˆä»å‡ ä¹ä¸º 0 æå‡åˆ°å¯æ¥å—æ°´å¹³ï¼‰
- **æ„ä¹‰**ï¼šæ¨¡å‹å¼€å§‹çœŸæ­£è¯†åˆ«æƒ…æ„Ÿå€¾å‘ï¼Œè€Œä¸æ˜¯ã€Œèººå¹³ã€é¢„æµ‹å¤šæ•°ç±»

**2. avg_dim_accuracy ä¸‹é™ âš ï¸**
- **åŸå› **ï¼šé¢„æµ‹å¤šæ ·åŒ–åï¼Œã€ŒçŒœå¯¹ã€ä¸­æ€§å˜å¾—å›°éš¾
- **è§£é‡Š**ï¼š
  - ä¼˜åŒ–å‰ï¼šæ¨¡å‹ 90% æ—¶é—´é¢„æµ‹"ä¸­æ€§"ï¼Œç¢°å·§ 76% æ ·æœ¬ç¡®å®æ˜¯ä¸­æ€§ï¼Œæ‰€ä»¥å‡†ç¡®ç‡é«˜
  - ä¼˜åŒ–åï¼šæ¨¡å‹å°è¯•è¯†åˆ«è´Ÿé¢/æ­£é¢ï¼Œä½†è¿™ä¸¤ä¸ªç±»åˆ«ç‰¹å¾æ›´å¤æ‚ï¼Œå®¹æ˜“è¯¯åˆ¤
- **ç»“è®º**ï¼šè¿™æ˜¯**å¥åº·çš„ä¸‹é™**ï¼Œè¯´æ˜æ¨¡å‹åœ¨å­¦ä¹ æ›´æœ‰æ„ä¹‰çš„å†³ç­–è¾¹ç•Œ

**3. ä¸¥æ ¼å‡†ç¡®ç‡ä¸‹é™ âš ï¸**
- **åŸå› **ï¼š18 ç»´å…¨éƒ¨æ­£ç¡®çš„æ¦‚ç‡æä½ï¼Œå½“æ¨¡å‹ä¸å†ã€Œä¿å®ˆã€é¢„æµ‹å…¨ä¸­æ€§æ—¶ï¼Œå…¨éƒ¨æ­£ç¡®çš„æ¦‚ç‡è‡ªç„¶é™ä½
- **æ„ä¹‰**ï¼šæ­¤æŒ‡æ ‡åœ¨æ­¤ä»»åŠ¡ä¸­**ä¸å…·å¤‡å‚è€ƒä»·å€¼**ï¼Œåº”å¿½ç•¥

#### ä¼˜åŒ–å»ºè®®çš„ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | ç­–ç•¥ | é¢„æœŸ F1-macro | ä»£ä»· |
|--------|------|---------------|------|
| P0 | åŠ æƒ Loss + è¿‡é‡‡æ ·ï¼ˆå·²åšï¼‰ | 40-45% | å•ç»´å‡†ç¡®ç‡ä¸‹é™ 5% |
| P1 | Focal Loss + å±‚ wise LRï¼ˆå·²å†…ç½®ï¼‰ | 45-50% | è®­ç»ƒæ—¶é—´å¢åŠ  |
| **P2** | **æ¢ L6 æ¨¡å‹ï¼ˆå·²å®ç°ï¼‰** | **57.5%** âœ… | æ¨¡å‹ 244MBï¼Œbatch_size éœ€é™åˆ° 32 |
| P3 | EDA æ•°æ®å¢å¼º | 58-62% | é¢„å¤„ç†æ—¶é—´å¢åŠ  |
| P4 | é›†æˆå­¦ä¹  | 60-65% | æ¨ç†æˆæœ¬ x3 |

### æ¨èçš„è§£å†³æ–¹æ¡ˆ

**æ–¹æ¡ˆ 1ï¼šç±»åˆ«åŠ æƒ Lossï¼ˆå·²å†…ç½®ï¼‰**
```python
# model.py ä¸­è‡ªåŠ¨ä½¿ç”¨åŠ æƒ Loss
class_weights = [5.0, 1.0, 2.5]  # [è´Ÿé¢, ä¸­æ€§, æ­£é¢]
```
æƒé‡åŸºäºé¢‘ç‡å€’æ•°è®¡ç®—ï¼Œè®©æ¨¡å‹æ›´å…³æ³¨å°‘æ•°ç±»ï¼ˆè´Ÿé¢ï¼‰ã€‚

**æ–¹æ¡ˆ 2ï¼šæ ·æœ¬å¹³è¡¡ï¼ˆæ¨èï¼‰**
```bash
# é¢„å¤„ç†æ—¶ä½¿ç”¨è¿‡é‡‡æ ·
python -m src.preprocess --balance oversample

# ç­–ç•¥é€‰é¡¹ï¼š
# - oversample: å¤åˆ¶å°‘æ•°ç±»æ ·æœ¬ï¼ˆæ¨èï¼‰
# - undersample: å‡å°‘å¤šæ•°ç±»æ ·æœ¬
# - hybrid: æ··åˆç­–ç•¥
```

**æ–¹æ¡ˆ 3ï¼šå…³æ³¨æ­£ç¡®çš„æŒ‡æ ‡**

| æŒ‡æ ‡ | æ­£å¸¸èŒƒå›´ | è¯´æ˜ |
|------|---------|------|
| **avg_dim_accuracy** | 75-85% | âœ… å•ç»´åº¦å‡†ç¡®ç‡ï¼Œæœ€å¯é  |
| **f1_macro** | 43-58% | è€ƒè™‘ç±»åˆ«ä¸å¹³è¡¡ï¼Œä¼˜åŒ–ç›®æ ‡ï¼ˆL6 å¯è¾¾ 57.5%ï¼‰ |
| strict_accuracy | 2-5% | 18ç»´å…¨å¯¹ï¼Œä»…ä½œå‚è€ƒ |

## ğŸš€ è„±èƒæ¢éª¨ï¼šå…¨æ–¹ä½æå‡æ–¹æ¡ˆï¼ˆå·²å®ç° F1-macro 57.5%ï¼‰

é€šè¿‡ç³»ç»Ÿæ€§ä¼˜åŒ–ï¼Œæˆ‘ä»¬å°† F1-macro ä» **43.0% (L4)** æå‡åˆ° **57.5% (L6)**ï¼Œæå‡å¹…åº¦è¾¾ **+34%**ï¼

### æœ€ç»ˆå®éªŒç»“æœå¯¹æ¯”

| æ¨¡å‹ | é…ç½® | F1-macro | avg_dim_accuracy | è®­ç»ƒæ—¶é—´ |
|------|------|----------|------------------|----------|
| **L4** | åŸºç¡€é…ç½® | **43.0%** | 72.5% | 25åˆ†é’Ÿ |
| **L4+** | +åŠ æƒLoss+è¿‡é‡‡æ · | **43.0%** | 72.5% | 30åˆ†é’Ÿ |
| **L6** | +Focal Loss+å±‚wise LR+Early Stopping | **57.5%** ğŸ† | 75.3% | 1å°æ—¶ |

**å…³é”®å‘ç°ï¼š**
- **æ¢ L6 æ¨¡å‹**æ˜¯æœ€å¤§æå‡æ¥æºï¼ˆ+14.5%ï¼‰
- **Focal Loss** æ¯”åŠ æƒ CE æ›´æœ‰æ•ˆå¤„ç†éš¾åˆ†ç±»æ ·æœ¬
- **å±‚ wise å­¦ä¹ ç‡**è®©å¾®è°ƒæ›´ç¨³å®š
- **Early Stopping** (patience=3) æœ‰æ•ˆé˜²æ­¢è¿‡æ‹Ÿåˆ

å¦‚æœä½ æƒ³å¤ç°æˆ–è¿›ä¸€æ­¥æå‡ï¼Œå®æ–½ä»¥ä¸‹æ–¹æ¡ˆï¼š

### æ–¹æ¡ˆ 1ï¼šæ¢æ›´å¤§çš„åº•åº§æ¨¡å‹ï¼ˆå·²å®ç°ï¼Œ+14.5%ï¼‰

TinyBERT-L4 åªæœ‰ 4 å±‚ï¼Œè¡¨è¾¾èƒ½åŠ›æœ‰é™ã€‚æ¢æˆ **L6ï¼ˆ6å±‚ï¼‰**åï¼š
- F1-macro ä» **43.0% â†’ 57.5%** âœ…
- æ¨¡å‹ä» 50MB â†’ 244MB
- æ˜¾å­˜éœ€æ±‚ä» 2.8GB â†’ 3.2GB
- batch_size éœ€ä» 80 é™åˆ° 32

```bash
# ä¸‹è½½ L6 æ¨¡å‹ï¼ˆåä¸ºè¯ºäºšæ–¹èˆŸç‰ˆæœ¬ï¼Œä¸­æ–‡ä¼˜åŒ–æ›´å¥½ï¼‰
# æ¥æº: https://huggingface.co/huawei-noah/TinyBERT_6L_zh

# æ”¾å…¥ç›®å½•: models/chinese-tinybert-l6-uncased/
# éœ€è¦æ–‡ä»¶: config.json, pytorch_model.bin (244MB), vocab.txt

# ä¿®æ”¹ configs/hyperparams.yaml
model:
  name: "./models/chinese-tinybert-l6-uncased"

training:
  per_device_train_batch_size: 32  # L6 éœ€è¦æ›´å¤šæ˜¾å­˜
  per_device_eval_batch_size: 64
```

### æ–¹æ¡ˆ 2ï¼šFocal Lossï¼ˆè§£å†³ç±»åˆ«ä¸å¹³è¡¡çš„æ ¸æ­¦å™¨ï¼‰

å·²å†…ç½®ï¼åœ¨ `model.py` ä¸­è‡ªåŠ¨ä½¿ç”¨ã€‚ç›¸æ¯”åŠ æƒ CEï¼ŒFocal Loss æ›´å…³æ³¨**éš¾åˆ†ç±»æ ·æœ¬**ã€‚

### æ–¹æ¡ˆ 3ï¼šEDA æ•°æ®å¢å¼ºï¼ˆç”Ÿæˆå¤šæ ·åŒ–æ ·æœ¬ï¼‰

ä¸åªæ˜¯å¤åˆ¶æ ·æœ¬ï¼Œè¿˜è¦**åŒä¹‰è¯æ›¿æ¢ã€éšæœºæ’å…¥ã€éšæœºäº¤æ¢ã€éšæœºåˆ é™¤**ï¼š

```bash
# å…ˆè¿è¡Œ EDA å¢å¼º
python -c "
from src.data_augmentation import augment_dataset_with_eda
augment_dataset_with_eda(
    'data/processed/train_balanced.jsonl',
    'data/processed/train_eda.jsonl',
    target_size=100000  # ç›®æ ‡ 10 ä¸‡æ¡
)
"

# ç„¶åç”¨å¢å¼ºåçš„æ•°æ®è®­ç»ƒ
cp data/processed/train_eda.jsonl data/processed/train.jsonl
python -m src.train
```

### æ–¹æ¡ˆ 4ï¼šå±‚ wise å­¦ä¹ ç‡ + æ›´å¤š Epochs + Early Stopping

é…ç½®å·²æ›´æ–°ï¼š
- `num_train_epochs: 15`ï¼ˆåŸæ¥æ˜¯ 5ï¼‰
- `early_stopping_patience: 3`ï¼ˆæ—©åœé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
- `layer_wise_lr_decay: 0.9`ï¼ˆåº•å±‚å­¦ä¹ ç‡å°ï¼Œé¡¶å±‚å¤§ï¼‰
- `learning_rate: 3.0e-5`ï¼ˆç•¥å¾®æå‡ï¼‰

### å®Œæ•´æµç¨‹ï¼ˆè„±èƒæ¢éª¨ç‰ˆï¼‰

```bash
# 1. æ¸…ç†
rm -rf data/processed/*

# 2. é¢„å¤„ç† + è¿‡é‡‡æ ·
python -m src.preprocess --balance oversample

# 3. EDA å¢å¼ºï¼ˆå¯é€‰ï¼Œæ•ˆæœæ˜¾è‘—ä½†è€—æ—¶ï¼‰
python -c "
from src.data_augmentation import augment_dataset_with_eda
augment_dataset_with_eda(
    'data/processed/train_balanced.jsonl',
    'data/processed/train.jsonl'
)
"

# 4. è®­ç»ƒï¼ˆä¼šè‡ªåŠ¨ä½¿ç”¨ Focal Loss + å±‚ wise LR + Early Stoppingï¼‰
python -m src.train

# å®é™…ç»“æœï¼ˆL6 æ¨¡å‹ï¼‰ï¼š
# - F1-macro: 57.5% (ç¬¬9 epoch æœ€ä½³)
# - avg_dim_accuracy: 75.3%
# - ä¸¥æ ¼å‡†ç¡®ç‡: 1.8%
# - è®­ç»ƒæ—¶é—´: ~1 å°æ—¶ï¼ˆ12 epochsï¼ŒEarly Stoppingï¼‰
# - æ€»è®­ç»ƒæ­¥æ•°: 30,180

# 5. å¯¼å‡º ONNX
python -m deployments.export_onnx --model_path checkpoints/best_model
# è¾“å‡º: deployments/model.onnx (244MB)

# 6. INT8 é‡åŒ–
python -m deployments.quantize --benchmark
# è¾“å‡º: deployments/model_int8.onnx (~60MB)
# æ³¨æ„: å¦‚é‡ optimize_model å‚æ•°é”™è¯¯ï¼Œè¯·æ›´æ–°ä»£ç ï¼ˆè§"éƒ¨ç½²æ³¨æ„äº‹é¡¹"ï¼‰

# 7. æ¨ç†æµ‹è¯•
python -m deployments.predictor \
    --model_path deployments/model_int8.onnx \
    --text "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œéå¸¸æ»¡æ„ï¼"

# å®é™…æ¨ç†æ€§èƒ½ï¼ˆCPUï¼‰ï¼š
# - æ¨ç†å»¶è¿Ÿ: ~13 ms
# - ååé‡: 70 QPS
# - æ³¨æ„: å¦‚é‡ CUDA é”™è¯¯ï¼Œä¼šè‡ªåŠ¨å›é€€åˆ° CPUï¼ˆè§"éƒ¨ç½²æ³¨æ„äº‹é¡¹"ï¼‰
```

### æç«¯æ–¹æ¡ˆï¼šé›†æˆå­¦ä¹ ï¼ˆå† +3-5%ï¼‰

è®­ç»ƒ 3 ä¸ªä¸åŒéšæœºç§å­çš„æ¨¡å‹ï¼Œé¢„æµ‹æ—¶æŠ•ç¥¨ï¼š

```bash
# è®­ç»ƒ 3 ä¸ªæ¨¡å‹
for seed in 42 123 456; do
    python -m src.train --seed $seed
done
```

## ğŸ”§ é…ç½®è¯´æ˜

`configs/hyperparams.yaml` å…³é”®å‚æ•°ï¼ˆL6 æ¨èé…ç½®ï¼‰ï¼š

```yaml
model:
  name: "./models/chinese-tinybert-l6-uncased"  # L6 æ¨¡å‹è·¯å¾„
  max_length: 128          # æœ€å¤§åºåˆ—é•¿åº¦
  num_labels: 18           # ASAP ç»´åº¦æ•°
  use_weighted_loss: true  # å¯ç”¨ Focal Loss
  class_weights: [5.0, 1.0, 2.5]  # [è´Ÿé¢, ä¸­æ€§, æ­£é¢]

training:
  per_device_train_batch_size: 32   # L6 æ˜¾å­˜éœ€æ±‚å¤§ï¼Œbatch_size é™åˆ° 32
  per_device_eval_batch_size: 64
  learning_rate: 3.0e-5             # ç•¥å¾®æå‡
  num_train_epochs: 15              # æ›´å¤š epochs
  early_stopping_patience: 3        # æ—©åœé˜²æ­¢è¿‡æ‹Ÿåˆ
  layer_wise_lr_decay: 0.9          # å±‚ wise å­¦ä¹ ç‡è¡°å‡
  fp16: true                        # æ··åˆç²¾åº¦

rdrop:
  enabled: true
  alpha: 5.0               # R-Drop æƒé‡
```

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### é’ˆå¯¹ 3050Ti ä¼˜åŒ–

1. **batch_size**: è®¾ç½®ä¸º 16ï¼Œé¿å… OOM
2. **max_length**: 128 è¶³å¤Ÿè¦†ç›– 95% çš„æ ·æœ¬
3. **fp16**: å¿…é¡»å¼€å¯ï¼ŒèŠ‚çœæ˜¾å­˜
4. **gradient_checkpointing**: å¦‚éœ€æ›´å¤§ batchï¼Œå¯å¼€å¯

### æ•°æ®å¢å¼ºï¼ˆå¯é€‰ï¼‰

ä½¿ç”¨ LLM ç”Ÿæˆè§£é‡Šæ€§ä¼ªæ ‡ç­¾ï¼š

```bash
python -m src.augment \
    --mode augment \
    --input_path data/processed/train.jsonl \
    --output_path data/processed/train_augmented.jsonl \
    --model_name qwen2-7b-instruct
```

### å™ªå£°è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰

ä½¿ç”¨ LLM-as-a-Judge æ¸…æ´—è„æ•°æ®ï¼š

```bash
python -m src.augment \
    --mode filter \
    --input_path data/processed/train.jsonl \
    --output_path data/processed/train_cleaned.jsonl
```

## ğŸ“ API ç¤ºä¾‹

```python
from deployments.predictor import SentimentPredictor

# åˆ›å»ºé¢„æµ‹å™¨
predictor = SentimentPredictor(
    model_path='deployments/model_int8.onnx',
    use_tensorrt=True
)

# å•æ¡é¢„æµ‹
result = predictor.predict("è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼")
print(f"æƒ…æ„Ÿï¼š{result.overall_sentiment}")
print(f"ç½®ä¿¡åº¦ï¼š{result.confidence:.3f}")

# æ‰¹é‡é¢„æµ‹
texts = ["å¥½è¯„", "å·®è¯„", "ä¸€èˆ¬èˆ¬"]
results = predictor.predict_batch(texts)
```

## ğŸ“Š å®Œæ•´å®éªŒè®°å½•

### å®éªŒç¯å¢ƒ
- **GPU**: NVIDIA RTX 3050Ti (4GB)
- **Python**: 3.10
- **PyTorch**: 2.0+
- **CUDA**: 11.8

### å®éªŒ 1: L4 åŸºçº¿æ¨¡å‹
```bash
# é…ç½®
model: chinese-tinybert-l4-uncased (4å±‚, 50MB)
batch_size: 80
epochs: 5

# ç»“æœ
F1-macro: 36.7% (åŸºç¡€) â†’ 43.0% (åŠ æƒLoss+è¿‡é‡‡æ ·)
avg_dim_accuracy: 78.7%
è®­ç»ƒæ—¶é—´: 25åˆ†é’Ÿ
```

### å®éªŒ 2: L6 è„±èƒæ¢éª¨æ¨¡å‹ âœ… æœ€ç»ˆæ–¹æ¡ˆ

**è®­ç»ƒé˜¶æ®µï¼š**
```bash
# é…ç½®
model: chinese-tinybert-l6-uncased (6å±‚, 244MB)
ä¼˜åŒ–ç­–ç•¥: Focal Loss + å±‚wise LR + è¿‡é‡‡æ · + Early Stopping
batch_size: 32 (L6 æ˜¾å­˜éœ€æ±‚æ›´å¤§)
epochs: 15 (Early Stopping patience=3)

# è®­ç»ƒç»“æœ
æœ€ä½³ F1-macro: 57.5% (ç¬¬9 epoch) ğŸ†
æœ€ç»ˆ F1-macro: 57.0% (ç¬¬12 epochï¼ŒEarly Stopping)
avg_dim_accuracy: 75.3%
ä¸¥æ ¼å‡†ç¡®ç‡: 1.8%
è®­ç»ƒæ­¥æ•°: 30,180
è®­ç»ƒæ—¶é—´: ~1å°æ—¶
```

**éƒ¨ç½²é˜¶æ®µï¼š**
```bash
# 1. ONNX å¯¼å‡º
python -m deployments.export_onnx --model_path checkpoints/best_model
# è¾“å‡º: deployments/model.onnx (244MB)

# 2. INT8 é‡åŒ–
python -m deployments.quantize --benchmark
# è¾“å‡º: deployments/model_int8.onnx (~60MB)
# é—®é¢˜: quantize_dynamic() çš„ optimize_model å‚æ•°é”™è¯¯
# è§£å†³: åˆ é™¤è¯¥å‚æ•°ï¼ˆè§"éƒ¨ç½²æ³¨æ„äº‹é¡¹"ï¼‰

# 3. æ¨ç†æµ‹è¯•
python -m deployments.predictor --benchmark
# é—®é¢˜: CUDA/cuDNN ç‰ˆæœ¬ä¸åŒ¹é…ï¼ŒGPU æ¨ç†å¤±è´¥
# è§£å†³: è‡ªåŠ¨å›é€€åˆ° CPUï¼Œæ€§èƒ½å¯æ¥å—

# éƒ¨ç½²ç»“æœ
æ¨¡å‹å¤§å°: 244MB â†’ 60MB (INT8 é‡åŒ–ï¼Œå‹ç¼© 75%)
æ¨ç†å»¶è¿Ÿ: ~13 ms (CPU)
ååé‡: 70 QPS
```

### å…³é”®ç»“è®º

**è®­ç»ƒä¼˜åŒ–ï¼š**
1. **L6 æ¢æ¨¡å‹**æ˜¯æœ€å¤§æå‡æ¥æº: +14.5%
2. **Focal Loss** æ¯”åŠ æƒ CE æ›´æœ‰æ•ˆ: ä¸“æ³¨éš¾åˆ†ç±»æ ·æœ¬
3. **å±‚ wise å­¦ä¹ ç‡**: åº•å±‚å°ã€é¡¶å±‚å¤§ï¼Œå¾®è°ƒæ›´ç¨³å®š
4. **Early Stopping**: patience=3 æœ‰æ•ˆé˜²æ­¢è¿‡æ‹Ÿåˆ
5. **ä¸¥æ ¼å‡†ç¡®ç‡ä¸‹é™**æ˜¯æ­£å¸¸çš„: æ¨¡å‹ä¸å†ã€Œèººå¹³ã€é¢„æµ‹ä¸­æ€§

**éƒ¨ç½²ç»éªŒï¼š**
6. **INT8 é‡åŒ–**: æ¨¡å‹ä» 244MB å‹ç¼©åˆ° 60MBï¼Œä½“ç§¯å‡å°‘ 75%
7. **CPU æ¨ç†**: L6 æ¨¡å‹ CPU å»¶è¿Ÿ ~13msï¼Œååé‡ 70 QPSï¼Œè¶³å¤Ÿç”Ÿäº§ä½¿ç”¨
8. **GPU æ¨ç†å‘**: ONNX Runtime å¯¹ CUDA/cuDNN ç‰ˆæœ¬è¦æ±‚ä¸¥æ ¼ï¼Œå»ºè®®ç›´æ¥ç”¨ CPU
9. **é‡åŒ–å‘**: æ–°ç‰ˆæœ¬ ONNX Runtime ç§»é™¤äº† optimize_model å‚æ•°ï¼Œéœ€è¦æ‰‹åŠ¨ä¿®å¤

### å¤ç°æœ€ä½³ç»“æœ

```bash
# 1. å‡†å¤‡ L6 æ¨¡å‹
# ä¸‹è½½ https://huggingface.co/huawei-noah/TinyBERT_6L_zh
# æ”¾å…¥ models/chinese-tinybert-l6-uncased/

# 2. é¢„å¤„ç†
python -m src.preprocess --balance oversample

# 3. è®­ç»ƒ
python -m src.train
# é¢„æœŸ: F1-macro 55-58%

# 4. å¯¼å‡ºä¸éƒ¨ç½²
python -m deployments.export_onnx --model_path checkpoints/best_model
python -m deployments.quantize --benchmark
python -m deployments.predictor --text "æµ‹è¯•æ–‡æœ¬"
# æ³¨æ„: å¦‚é‡éƒ¨ç½²é—®é¢˜ï¼Œè¯·æŸ¥çœ‹"éƒ¨ç½²æ³¨æ„äº‹é¡¹"ç« èŠ‚
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ PRï¼

## ğŸ“„ è®¸å¯è¯

MIT License
